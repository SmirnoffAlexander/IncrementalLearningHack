{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "#set seeds\n",
    "\n",
    "def get_models(_config):\n",
    "    pretrained_weights_path = _config['pretrained_weights_path']\n",
    "    device = _config['device']\n",
    "    state_dict = torch.load(pretrained_weights_path)\n",
    "    out_features, in_features = state_dict['fc.weight'].shape\n",
    "    model = torchvision.models.resnet50(pretrained=False)\n",
    "    old_model = torchvision.models.resnet50(pretrained=False)\n",
    "    model.fc = torch.nn.Linear(in_features, out_features)\n",
    "    old_model.fc = torch.nn.Linear(in_features, out_features)\n",
    "    model.load_state_dict(state_dict)\n",
    "    old_model.load_state_dict(state_dict)\n",
    "    old_model.eval()\n",
    "    \n",
    "    return model, old_model\n",
    "\n",
    "def get_loss(_config):\n",
    "    if _config['loss'] == \"CrossEntropy\":\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "    return loss\n",
    "\n",
    "def seed_all(seed):\n",
    "\tnp.random.seed(seed)\n",
    "\ttorch.manual_seed(seed)\n",
    "\ttorch.cuda.manual_seed_all(seed)\n",
    "\trandom.seed(seed)\n",
    "\ttorch.backends.cudnn.benchmark = False\n",
    "\ttorch.backends.cudnn.deterministic = True\n",
    "\ttorch.backends.cudnn.enabled = True\n",
    " \n",
    "def kaiming_normal_init(m):\n",
    "\tif isinstance(m, nn.Conv2d):\n",
    "\t\tnn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "\telif isinstance(m, nn.Linear):\n",
    "\t\tnn.init.kaiming_normal_(m.weight, nonlinearity='sigmoid')\n",
    "\n",
    "\n",
    "_config = {\n",
    "    'data_path': 'horse',\n",
    "    'num_epochs': 100,\n",
    "\t'T': 2,\n",
    "\t'alpha': 0.01,\n",
    "\t'num_new_class': 1,\n",
    "\t'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "\t'lr': 1e-2,\n",
    "\t'momentum': 0.9,\n",
    "\t'weight_decay': 5e-4,\n",
    "\t'optimizer': 'SGD',\n",
    "\t'pretrained_weights_path': 'pretrained_weights/net.pth',\n",
    "\t'loss': 'CrossEntropy',\n",
    "\t'batch_size': 4,\n",
    "\t'num_workers': 4,\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, ds_type='train', data_path=None, imgs_list=None):\n",
    "        self.ds_type = ds_type\n",
    "        if ds_type == 'train':\n",
    "            self.transforms = transforms.Compose([transforms.Resize(size=(224,224)),\n",
    "                                                  transforms.RandomHorizontalFlip(),\n",
    "                                                  transforms.ColorJitter(brightness=0.4, saturation=0.4, hue=0.4),\n",
    "                                                  transforms.RandomRotation(degrees=30), \n",
    "                                                  transforms.ToTensor(),\n",
    "                                                  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        else:\n",
    "            self.transforms = transforms.Compose([transforms.Resize(size=(224,224)),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "        \n",
    "        self.data_path = data_path # path to dir with images\n",
    "        self.class_name = self.data_path.split('/')[-1]\n",
    "        self.images_names = imgs_list\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images_names)\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.images_names[index]\n",
    "        img_path = os.path.join(self.data_path, img_name)\n",
    "        label = torch.tensor(0)\n",
    "        img = Image.open(img_path)\n",
    "        img = self.transforms(img)\n",
    "        return img, label\n",
    "        \n",
    "        \n",
    "def get_dataloader(train_imgs_list, test_imgs_list, _config):\n",
    "    train_dataset = CustomDataset(ds_type='train', data_path=_config['data_path'], imgs_list=train_imgs_list)\n",
    "    test_dataset =  CustomDataset(ds_type='test', data_path=_config['data_path'], imgs_list=test_imgs_list)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=_config['batch_size'], num_workers=_config['num_workers'], shuffle=True)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=_config['batch_size'], num_workers=_config['num_workers'], shuffle=False)\n",
    "    return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(_config):\n",
    "    num_new_class = _config['num_new_class']\n",
    "    device = _config['device']\n",
    "    model, old_model = get_models(_config)\n",
    "    in_features = model.fc.in_features\n",
    "    out_features = model.fc.out_features\n",
    "    weight = model.fc.weight.data\n",
    "    bias = model.fc.bias.data\n",
    "    new_out_features = out_features + num_new_class\n",
    "    new_fc = nn.Linear(in_features, new_out_features)\n",
    "    kaiming_normal_init(new_fc)\n",
    "    new_fc.weight.data[:out_features] = weight\n",
    "    new_fc.bias.data[:out_features] = bias\n",
    "    model.fc = new_fc\n",
    "    model = model.to(device)\n",
    "    old_model = old_model.to(device)\n",
    "    cudnn.benchmark = True\n",
    "    return model, old_model\n",
    "\n",
    "\n",
    "def get_optimizer(model, _config):\n",
    "    if _config['optimizer'] == 'SGD':\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=_config['lr'],\n",
    "                                    momentum=_config['momentum'], weight_decay=_config['weight_decay'])\n",
    "    if _config['optimizer'] == 'AdamW':\n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=_config['lr'], weight_decay=_config['weight_decay'])\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch, model, old_model, train_loader, optimizer, criterion, _config):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    device = _config['device']\n",
    "    T = _config['T']\n",
    "    alpha = _config['alpha']\n",
    "    out_features  = old_model.fc.out_features\n",
    "    pbar = tqdm(train_loader)\n",
    "    for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        targets += out_features\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        soft_target = old_model(inputs)\n",
    "        loss1 = criterion(outputs, targets)\n",
    "        \n",
    "        outputs_S = F.softmax(outputs[:,:out_features]/T,dim=1)\n",
    "        outputs_T = F.softmax(soft_target[:,:out_features]/T,dim=1)\n",
    "        \n",
    "        loss2 = outputs_T.mul(-1*torch.log(outputs_S))\n",
    "        loss2 = loss2.sum(1)\n",
    "        loss2 = loss2.mean()*T*T\n",
    "        loss = loss1*alpha+loss2*(1-alpha)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        pbar.set_description(f\"Epoch: {epoch} | Loss: {train_loss / (batch_idx+1)} | Acc: {100. * correct /total}\")\n",
    "        \n",
    "    return train_loss / (batch_idx+1)\n",
    "\n",
    "\n",
    "def eval_one_epoch(epoch, model, test_dataloader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pass\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "def create_markup(_config):\n",
    "    imgs_list = os.listdir(_config['data_path'])\n",
    "    n = len(imgs_list)\n",
    "    train_len = int(0.9*n)\n",
    "    random.shuffle(imgs_list)\n",
    "    train_imgs_list = imgs_list[:train_len]\n",
    "    test_imgs_list = imgs_list[train_len:]\n",
    "    return train_imgs_list, test_imgs_list\n",
    "    \n",
    "def run_experiment(_config):\n",
    "    model, old_model = prepare_model(_config)\n",
    "    train_imgs_list, test_imgs_list = create_markup(_config)\n",
    "    train_dataloader, val_dataloader = get_dataloader(train_imgs_list, test_imgs_list, _config)\n",
    "    optimizer = get_optimizer(model, _config)\n",
    "    \n",
    "    loss = get_loss(_config)\n",
    "    for epoch in range(_config['num_epochs']):\n",
    "        train_one_epoch(epoch, model, old_model, train_dataloader, optimizer, loss, _config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexander/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/alexander/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "  0%|          | 0/7 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [3], line 93\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(_config)\u001b[0m\n\u001b[1;32m     91\u001b[0m loss \u001b[38;5;241m=\u001b[39m get_loss(_config)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(_config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 93\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mold_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [3], line 56\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch, model, old_model, train_loader, optimizer, criterion, _config)\u001b[0m\n\u001b[1;32m     54\u001b[0m loss2 \u001b[38;5;241m=\u001b[39m loss2\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m*\u001b[39mT\u001b[38;5;241m*\u001b[39mT\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss1\u001b[38;5;241m*\u001b[39malpha\u001b[38;5;241m+\u001b[39mloss2\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39malpha)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_experiment(_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
